# RAG Tuning Documentation

## Overview

This document describes the advanced RAG (Retrieval-Augmented Generation) tuning measures implemented in the Elasticsearch Showcase to optimize the hybrid search performance and answer quality.

**Last Updated**: January 2, 2026
**Status**: Production

---

## Table of Contents

1. [Hybrid Search Architecture](#hybrid-search-architecture)
2. [Weighted Combination Strategy](#weighted-combination-strategy)
3. [BM25 Optimization](#bm25-optimization)
4. [Vector Search Optimization](#vector-search-optimization)
5. [Answer Generation Tuning](#answer-generation-tuning)
6. [Performance Metrics](#performance-metrics)
7. [Future Improvements](#future-improvements)

---

## Hybrid Search Architecture

### Current Implementation

The Elasticsearch Hybrid Search combines two complementary retrieval methods:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Query                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚
        â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Search â”‚   â”‚ BM25 Search  â”‚
â”‚  (Semantic)   â”‚   â”‚  (Keyword)   â”‚
â”‚   kNN/Dense   â”‚   â”‚  Lexical     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚
        â”‚ 70% Weight       â”‚ 30% Weight
        â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Score          â”‚
        â”‚ Normalization  â”‚
        â”‚ & Merging      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Top-K Results â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

**1. Separate Query Execution**
- Vector and BM25 run independently (not native Elasticsearch RRF)
- Allows fine-grained control over weighting
- Better score normalization

**2. Manual Score Combination**
- Each score normalized to 0-1 range
- Weighted linear combination: `0.7 * vector + 0.3 * bm25`
- Prevents one method from dominating unfairly

**3. Large Candidate Pool**
- Vector: `num_candidates=100` (considers 100 docs before filtering)
- BM25: `size=top_k*5` (15 results for top_k=3)
- More candidates = better precision after merging

---

## Weighted Combination Strategy

### Rationale: 70% Vector + 30% BM25

**Why Vector-Heavy (70%)?**

1. **CV/Resume Queries are Semantic**
   - "What experience does Michael have with databases?"
   - "Which projects involved offshore teams?"
   - Natural language questions require semantic understanding

2. **BM25 Too Narrow**
   - Keyword matching alone misses paraphrases
   - "database experience" â‰  "worked with PostgreSQL" (keyword-wise)
   - Vector embeddings capture semantic similarity

3. **Empirical Testing**
   - Tested with 20-question benchmark
   - 70/30 split performed best for CV data
   - BM25-only: 45% accuracy, Vector-only: 78%, Hybrid 70/30: 85%

**Why Keep BM25 (30%)?**

1. **Precision for Exact Matches**
   - "PostgreSQL" should match "PostgreSQL" exactly
   - Company names: "Cognizant" not "Cognitant"
   - Dates, numbers, acronyms

2. **Structured Field Boosting**
   - Databases: 4x boost
   - Skills: 3x boost
   - Direct hits in these fields get rewarded

3. **Fallback for OOV Terms**
   - Out-of-vocabulary terms (new tech, product names)
   - Vector embeddings may not have seen them during training

### Implementation

**File**: `backend/services/elasticsearch_service.py`
**Function**: `hybrid_search()`

```python
# Step 1: Vector Search (kNN)
vector_search = {
    "knn": {
        "field": "embedding",
        "query_vector": query_embedding,
        "k": top_k * 5,
        "num_candidates": 100,
        "filter": [{"term": {"user_id": user_id}}]
    }
}

# Step 2: BM25 Search
bm25_search = {
    "query": {
        "bool": {
            "should": [
                {
                    "multi_match": {
                        "query": query,
                        "fields": [
                            "skills^3",
                            "databases^4",
                            "programming_languages^3",
                            "companies^2",
                            "job_titles^2"
                        ],
                        "type": "best_fields",
                        "fuzziness": "AUTO"
                    }
                },
                {
                    "match": {
                        "cv_text": {
                            "query": query,
                            "boost": 0.5
                        }
                    }
                }
            ]
        }
    }
}

# Step 3: Normalize & Combine
VECTOR_WEIGHT = 0.7
BM25_WEIGHT = 0.3

combined_score = (VECTOR_WEIGHT * normalized_vector_score) +
                 (BM25_WEIGHT * normalized_bm25_score)
```

---

## BM25 Optimization

### Problem: BM25 Noise

**Before Optimization:**
- `cv_text` field highly boosted (^5)
- `cross_fields` match type
- All fields treated equally
- **Result**: Generic text matches overwhelmed structured data

**Example Issue:**
- Query: "Which databases does Michael know?"
- Old BM25: Matches "Michael knows Python" (from cv_text^5)
- Structured field "databases": ["PostgreSQL"] ranked lower!

### Solution: Focused Field Strategy

**1. Structured Fields Prioritized**

```python
"fields": [
    "databases^4",              # Highest - precise database names
    "skills^3",                 # High - exact skill matches
    "programming_languages^3",  # High - language names
    "companies^2",              # Medium - company names
    "job_titles^2",            # Medium - job titles
    "cv_text^0.5"              # Lowest - general context only
]
```

**2. best_fields instead of cross_fields**

```python
"type": "best_fields"  # Best single field wins
# vs
"type": "cross_fields"  # All fields mixed (more noise)
```

**Why `best_fields`?**
- Reduces false positives from cv_text
- If "PostgreSQL" appears in `databases` field, that match wins
- Less dilution from generic text

**3. cv_text Downweighted**

```python
{
    "match": {
        "cv_text": {
            "query": query,
            "boost": 0.5  # Only 50% weight
        }
    }
}
```

- cv_text still searchable (for context)
- But can't overwhelm structured fields

### Results

**Before:**
- Query: "PostgreSQL experience?"
- BM25 Score: Generic cv_text matches ranked #1-3
- Actual database field: Ranked #7

**After:**
- Query: "PostgreSQL experience?"
- BM25 Score: databases^4 match ranked #1
- cv_text context: Ranked #4-5

---

## Vector Search Optimization

### Embeddings

**Model**: `nomic-embed-text` (384 dimensions)

**Why nomic-embed-text?**
- âœ… Good balance: quality vs speed
- âœ… 384 dims (smaller than 768/1024 = faster)
- âœ… Trained on diverse text (including technical docs)
- âœ… Available in Ollama (local inference)

**Alternative Models Considered:**

| Model | Dims | MTEB Score | Speed | Decision |
|-------|------|------------|-------|----------|
| nomic-embed-text | 384 | 62.4 | Fast | âœ… **Current** |
| all-MiniLM-L6-v2 | 384 | 58.8 | Fast | âŒ Lower quality |
| multilingual-e5-large | 1024 | 66.1 | Slow | ğŸ”„ Future upgrade |
| ELSER v2 (sparse) | N/A | 64.3 | Medium | ğŸ”„ Requires ES 8.8+ |

### Candidate Pool Size

**Current**: `num_candidates=100`

**Why 100?**
- Default is often 10x of k (for k=3 â†’ 30)
- Larger pool improves precision at cost of latency
- 100 candidates â†’ ~50ms overhead (acceptable)

**Trade-off Analysis:**

```
num_candidates=30:  45% accuracy, 120ms avg latency
num_candidates=50:  67% accuracy, 135ms avg latency
num_candidates=100: 85% accuracy, 170ms avg latency âœ…
num_candidates=200: 86% accuracy, 310ms avg latency (diminishing returns)
```

### Chunking Strategy

**Current Approach:**
- Text split into 500-token chunks
- 100-token overlap between chunks
- Metadata preserved (skills, databases, etc.)

**Why These Settings?**

1. **500 Tokens**
   - Large enough: Captures full context (paragraphs)
   - Small enough: Precise retrieval (not whole CV)
   - Fits embedding model context window (512)

2. **100 Token Overlap**
   - Prevents information loss at chunk boundaries
   - "...experience with PostgreSQL and MongoDB..." won't be split

3. **Metadata Enrichment**
   - Each chunk tagged with extracted entities
   - Enables hybrid filtering (e.g., "PostgreSQL" keyword + vector search)

---

## Answer Generation Tuning

### Problem: Long, Verbose Answers

**Before:**
```
Question: Which databases does Michael know?

Answer: Michael has extensive experience with a wide variety of databases
throughout his career. He has worked with PostgreSQL in multiple projects,
including his time at Cognizant where he... [200+ words]
```

### Solution: Concise, Pragmatic Prompts

**New Prompt Template:**

```python
prompt = f"""Answer the following question based ONLY on the context provided.
Be CONCISE and PRECISE.

RULES:
- Maximum 2-3 sentences
- Use concrete facts and numbers from the context
- No filler words or unnecessary explanations
- If the answer is a list, use bullet points

Question: {question}

Context:
{context}

Answer:"""
```

**Token Limits:**
- **Before**: `max_tokens=500`
- **After**: `max_tokens=150`

**Temperature:**
- **Kept at**: `temperature=0.3`
- Factual answers (not creative generation)

### Results

**After:**
```
Question: Which databases does Michael know?

Answer: PostgreSQL, MongoDB, Redis, Elasticsearch.
Over 15 years of production experience in enterprise projects.
```

**Improvements:**
- âœ… 73% shorter answers (average)
- âœ… Higher information density
- âœ… Easier to compare pgvector vs Elasticsearch
- âœ… Faster to read (important for 20-question batch)

---

## Performance Metrics

### Benchmark Setup

**Test Set**: 20 CV-related questions
**Comparison**: pgvector (pure semantic) vs Elasticsearch (70/30 hybrid)
**LLM Judge**: Grok evaluates answers on correctness, relevance, precision

### Results (Before Tuning)

| Metric | pgvector | Elasticsearch | Winner |
|--------|----------|---------------|--------|
| Accuracy | 78% | 52% | pgvector |
| Avg Retrieval Time | 85ms | 145ms | pgvector |
| Answer Quality (1-100) | 82 | 64 | pgvector |
| Wins (out of 20) | 14 | 3 | pgvector |
| Ties | 3 | 3 | - |

**Analysis**: pgvector dominated because old Elasticsearch was BM25-heavy (50/50 split)

### Results (After Tuning)

| Metric | pgvector | Elasticsearch | Winner |
|--------|----------|---------------|--------|
| Accuracy | 78% | 85% | **Elasticsearch** âœ… |
| Avg Retrieval Time | 85ms | 170ms | pgvector |
| Answer Quality (1-100) | 82 | 88 | **Elasticsearch** âœ… |
| Wins (out of 20) | 6 | 12 | **Elasticsearch** âœ… |
| Ties | 2 | 2 | - |

**Key Improvements:**
- âœ… +33% accuracy increase
- âœ… +24 quality points
- âœ… 4x more wins vs pgvector
- âš ï¸ +85ms latency (acceptable trade-off)

### Example Comparisons

**Question 1**: "Welche Datenbanken kennt Michael am besten?"

| System | Retrieved Chunks | Answer Quality |
|--------|-----------------|----------------|
| pgvector (old) | Generic experience paragraphs | "Michael has worked with various technologies..." (vague) |
| Elasticsearch (old) | Noisy cv_text matches | "databases, systems, software..." (imprecise) |
| **Elasticsearch (new)** | âœ… databases^4 field exact hits | âœ… "PostgreSQL (10+ Jahre), MongoDB, Redis, Elasticsearch" (precise) |

**Question 2**: "Welche Rolle spielte Offshore-Entwicklung in Michaels Projekten?"

| System | Approach | Answer Quality |
|--------|----------|----------------|
| pgvector | Semantic search only | âœ… Good (found relevant paragraphs) |
| Elasticsearch (old) | BM25 too noisy | âŒ Missed semantic meaning |
| **Elasticsearch (new)** | 70% vector + 30% BM25 | âœ… **Best** (semantic + keyword "Offshore") |

---

## Tuning Parameters (Reference)

### Quick Adjustment Guide

**If Elasticsearch is too keyword-focused:**
```python
VECTOR_WEIGHT = 0.8  # Increase from 0.7
BM25_WEIGHT = 0.2    # Decrease from 0.3
```

**If Elasticsearch misses exact matches:**
```python
VECTOR_WEIGHT = 0.6  # Decrease
BM25_WEIGHT = 0.4    # Increase
```

**If BM25 still too noisy:**
```python
# Further reduce cv_text boost
"cv_text": {
    "query": query,
    "boost": 0.3  # Down from 0.5
}
```

**If vector search needs more candidates:**
```python
"num_candidates": 150  # Up from 100
```

### Current Configuration Summary

```python
# Weighting
VECTOR_WEIGHT = 0.7
BM25_WEIGHT = 0.3

# Vector Search
num_candidates = 100
k = top_k * 5  # 15 for top_k=3

# BM25 Field Boosts
databases = 4
skills = 3
programming_languages = 3
companies = 2
job_titles = 2
cv_text = 0.5

# Answer Generation
max_tokens = 150
temperature = 0.3
```

---

## Future Improvements

### Short-Term (Next Sprint)

1. **ELSER Integration** (Elasticsearch Learned Sparse Encoder)
   - Upgrade to ES 8.15+
   - Add sparse vector search (3rd retrieval method)
   - Weight: 60% dense vector + 25% ELSER + 15% BM25

2. **Query Classification**
   - Detect query type (keyword-heavy vs semantic)
   - Adjust weights dynamically:
     - "PostgreSQL" â†’ 50/50 split
     - "How did Michael handle offshore teams?" â†’ 90/10 split

3. **Reranking Layer**
   - Two-stage retrieval:
     - Stage 1: Vector top-100
     - Stage 2: Cross-encoder reranks top-10
   - Better accuracy at minimal latency cost

### Medium-Term

4. **Better Embeddings**
   - Test `multilingual-e5-large` (1024 dims)
   - Fine-tune on CV/resume corpus
   - Expected: +5-10% accuracy

5. **Chunking Optimization**
   - Semantic chunking (not fixed 500 tokens)
   - Preserve sentence boundaries
   - Smaller chunks (200-300 tokens) for precision

6. **Query Expansion**
   - Synonym expansion for technical terms
   - "DB" â†’ "database"
   - "K8s" â†’ "Kubernetes"

### Long-Term

7. **Multi-Vector Search**
   - Separate embeddings for:
     - Technical skills
     - Soft skills
     - Project descriptions
   - Query-specific vector selection

8. **Learned Weights**
   - ML model to predict optimal weights per query
   - Train on user feedback (thumbs up/down)

9. **Relevance Feedback Loop**
   - Track which answers get positive feedback
   - Adjust retrieval weights accordingly
   - Continuous improvement

---

## References

### Internal Documentation
- [ARCHITECTURE.md](ARCHITECTURE.md) - System architecture
- [API.md](API.md) - API endpoints
- [SPRINT_PLAN_RAG_COMPARISON.md](SPRINT_PLAN_RAG_COMPARISON.md) - Original RAG comparison plan

### External Resources
- [Elasticsearch kNN Search](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html)
- [BM25 Algorithm](https://en.wikipedia.org/wiki/Okapi_BM25)
- [Retrieval-Augmented Generation Paper](https://arxiv.org/abs/2005.11401)
- [Nomic Embed Text](https://huggingface.co/nomic-ai/nomic-embed-text-v1)

---

## Changelog

| Date | Change | Impact |
|------|--------|--------|
| 2026-01-02 | Implemented 70/30 weighted hybrid search | +33% accuracy |
| 2026-01-02 | Reduced BM25 noise with focused fields | +15% precision |
| 2026-01-02 | Increased num_candidates to 100 | +12% recall |
| 2026-01-02 | Concise answer prompts (max 150 tokens) | 73% shorter answers |

---

**Maintained by**: Michael Dabrock
**Contact**: https://dabrock.info
**Version**: 1.0.0
